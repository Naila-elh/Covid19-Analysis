{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from py4j.java_gateway import java_import\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory  + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Local SparkSession\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print('Create Local SparkSession')\n",
    "    spark=SparkSession.builder.config(\"spark.driver.host\", \"localhost\").appName(\"clean_tweets\").getOrCreate()\n",
    "    \n",
    "# IgnoreCorruptFiles\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spark.sparkContext.getConf().getAll()\n",
    "#os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "path_to_data = \"../data/\"\n",
    "path_to_external_data = os.path.join(path_to_data, \"external-data/\")\n",
    "path_to_output = os.path.join(path_to_data,'visualisation_data')\n",
    "path_to_parquets = os.path.join(path_to_data,'chunks')\n",
    "parquet_files = sorted([os.path.join(path_to_parquets,'IDF-departments-to-analyze'),\n",
    "                        os.path.join(path_to_parquets, 'IDF-updates','**')])\n",
    "#parquet_files = [os.path.join(path_to_parquets, 'IDF-departments-to-analyze','**')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List files to be processed...\n",
      "# Files: 2\n"
     ]
    }
   ],
   "source": [
    "print('List files to be processed...')\n",
    "\n",
    "fs=spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "list_status=fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path_to_parquets))\n",
    "\n",
    "paths=[file.getPath().toString() for file in list_status]\n",
    "np.random.seed(0)\n",
    "paths=np.random.permutation(sorted(parquet_files))\n",
    "\n",
    "print('# Files:', len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets=spark.read.option(\"encoding\", \"UTF-8\").parquet(*parquet_files)\n",
    "tweets=tweets.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets : 42338679\n",
      "Number of unique users : 30651\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets : %d\" % tweets.count())\n",
    "print(\"Number of unique users : %d\" % tweets.select('user_id').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIDECODE : remove accents\n",
    "def make_trans():\n",
    "    matching_string = \"\"\n",
    "    replace_string = \"\"\n",
    "\n",
    "    for i in range(ord(\" \"), sys.maxunicode):\n",
    "        name = unicodedata.name(chr(i), \"\")\n",
    "        if \"WITH\" in name:\n",
    "            try:\n",
    "                base = unicodedata.lookup(name.split(\" WITH\")[0])\n",
    "                matching_string += chr(i)\n",
    "                replace_string += base\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    return matching_string, replace_string\n",
    "\n",
    "def clean_text(c):\n",
    "    matching_string, replace_string = make_trans()\n",
    "    return F.translate(\n",
    "        F.regexp_replace(c, \"\\p{M}\", \"\"), \n",
    "        matching_string, replace_string\n",
    "    ).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess tweets\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \n",
    "    df=df.select(\n",
    "            'id_str',\n",
    "            'user_id',\n",
    "            F.date_format(F.col('created_at'),\"yyyy-MM-dd\").alias('day').cast(\"date\"),\n",
    "            'full_text',\n",
    "            'lang'\n",
    "            )\n",
    "    \n",
    "    df = df.repartition(160)\n",
    "    \n",
    "    df = df.dropDuplicates(subset=['id_str'])\n",
    "    \n",
    "    # only after december 2019\n",
    "    df = df.filter(df.day>'2019-12-01')\n",
    "    \n",
    "    # remove rt\n",
    "    df = df.filter(~ df.full_text.startswith('RT'))\n",
    "    \n",
    "    # anonymization: identify userid, url, etc.\n",
    "#     pre_process_udf = F.udf(lambda x:text_processor.pre_process_doc(x))\n",
    "#     df=df.withColumn('clean_text', pre_process_udf('full_text'))\n",
    "    df = df.withColumn('clean_text', F.regexp_replace('full_text', r'@[A-Za-z0-9-_]+','@mention'))\n",
    "    df = df.withColumn('clean_text', F.regexp_replace('clean_text', 'https?://[A-Za-z0-9./]+','[url]'))\n",
    "\n",
    "    df=df.withColumn('anonymized_text', F.col('clean_text'))\n",
    "    \n",
    "    # Cleaning: tolower, remove punctuation\n",
    "    df=df.withColumn('clean_text', F.lower(F.col('clean_text')))\n",
    "    df=df.withColumn('clean_text', F.ltrim(df.clean_text))\n",
    " \n",
    "    # language : french\n",
    "    df = df.filter(df.lang=='fr')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = clean_dataset(tweets)\n",
    "#tweets = tweets.select('user_id','id_str','day', clean_text('full_text'),'lang', 'department')\n",
    "tweets = tweets.select('user_id','id_str','day','anonymized_text', clean_text('clean_text'),'lang')\n",
    "tweets = tweets.withColumn('clean_text', F.regexp_replace('clean_text', '[^\\sa-zA-Z0-9@]', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Symptoms analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Contains keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn('covid', F.col('clean_text').rlike('covid|corona |coronavirus'))\n",
    "tweets = tweets.withColumn('confinement', F.col('clean_text').rlike('confin|quarantaine'))\n",
    "tweets = tweets.withColumn('restezchezvous', F.col('clean_text').rlike('je reste chez moi|jerestechezmoi|restezchezvous|restez chez vous'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets mentioning COVID : 178673\n",
      "Number of tweets mentioning lockdown/quarantine : 171725\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets mentioning COVID : %d\" % tweets.filter(tweets.covid==1).count())\n",
    "print(\"Number of tweets mentioning lockdown/quarantine : %d\" % tweets.filter(tweets.confinement==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms_dict_fr = {'cough' : ['toux', 'tousse'],\n",
    "                   'sore_throat' : ['maux de gorge', 'mal de gorge', 'mal a la gorge'],\n",
    "                   'fever' : ['fievre', 'de la temperature'],\n",
    "                    #'mal de tête' : ['mal de tête','mal de crâne','mal à la tête','mal de tete','mal de crane','mal à la tete'],\n",
    "                   'loss_taste' : ['perte du gout', \"perte de lodorat\",\"perte de l odorat\",\"perdu l odorat\",\"perdu lodorat\",\n",
    "                                   \"perdu le gout\",\"plus de gout\",\"plus dodeur\",\"plus d odeur\"],\n",
    "                   'skin_symptom' : ['engelures'],\n",
    "                   'symptoms' : ['symptom'],\n",
    "                   'breathing_difficulties' : ['difficultes a respirer', 'difficultes respiratoires', 'difficulte a respirer',\n",
    "                                               'mal a respirer']\n",
    "                   #'hospitalisation' : ['hôpital','hopital','hospital','réanim','reanim']\n",
    "                   }\n",
    "\n",
    "for symptom in symptoms_dict_fr.keys():\n",
    "    tweets = tweets.withColumn(symptom, F.col('clean_text').rlike(('|').join(['^' + x for x in symptoms_dict_fr.get(symptom)] + \n",
    "                                                                            [' ' + x for x in symptoms_dict_fr.get(symptom)])))\n",
    "tweets = tweets.withColumn('nb_symptoms', sum(tweets[c].cast('long') for c in list(symptoms_dict_fr.keys())))\n",
    "tweets = tweets.withColumn('contains_symptom', F.col('nb_symptoms')>=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets mentionning symptoms : 9326\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets mentionning symptoms : %d\" % tweets.filter(tweets.contains_symptom==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Contains symptoms + pronums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FILTERS : if contains a pronum (marker of lived experience), or tweet startswith a symptom, \n",
    "# AND tweet does not contain hashtag\n",
    "\n",
    "# Indicators of feeling\n",
    "pronouns = ['g','j a','j ai','jai','m a','m ai','je','me', 'mes', 'l a', 'l ai','mon','ma','son','sa','jsui','j sui','j suis','jtousse']\n",
    "\n",
    "tweets = tweets.withColumn('pronoun', \n",
    "                           (F.col('clean_text').rlike(' |'.join(['^'+s for s in pronouns]))) |\n",
    "                          (F.col('clean_text').rlike((' |').join([' '+s for s in pronouns]))))\n",
    "\n",
    "# Startswith symptom\n",
    "tweets = tweets.withColumn('clean_text2', F.regexp_replace('clean_text','@mention', ''))\n",
    "tweets = tweets.withColumn('clean_text2', F.ltrim(tweets.clean_text2)) \\\n",
    "                .withColumn('start_symptom', (F.col('clean_text2').rlike('|'.join(['^'+s for s in list(symptoms_dict_fr.keys())]))))\n",
    "\n",
    "# Hashtags\n",
    "tweets = tweets.withColumn('hashtag', F.col('clean_text').contains('#'))\n",
    "\n",
    "# Classif\n",
    "tweets = tweets.withColumn('has_symptom', ((F.col('pronoun')==1) | (F.col('start_symptom')==1)) & \n",
    "                           (F.col('hashtag')==0) & (F.col('contains_symptom')==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets of people having symptoms (symptom+pronoun) : 4190\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets of people having symptoms (symptom+pronoun) : %d\" % tweets.filter(tweets.has_symptom==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Groupby day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mention_symptoms = tweets.filter(tweets['contains_symptom']==1)\n",
    "tweets_has_symptoms = tweets.filter(tweets['has_symptom']==1)\n",
    "tweets_covid_related = tweets.filter((tweets.covid==1)|(tweets.confinement==1)|(tweets.restezchezvous==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_covid=['covid','confinement','RestezChezVous']\n",
    "\n",
    "tweets_mention_symptoms = tweets_mention_symptoms.select(['day'] +\n",
    "                                                         [F.col(c).cast('long') for c in list(symptoms_dict_fr.keys())] +\n",
    "                                                         [F.col('contains_symptom').cast('long')])\\\n",
    "                                                .groupby('day').sum()\\\n",
    "                                                .orderBy('day')\\\n",
    "                                                .toDF(*['day']+list(symptoms_dict_fr.keys())+['has_symptom'])\n",
    "\n",
    "tweets_has_symptoms = tweets_has_symptoms.select(['day'] +\n",
    "                                                         [F.col(c).cast('long') for c in list(symptoms_dict_fr.keys())] +\n",
    "                                                         [F.col('has_symptom').cast('long')])\\\n",
    "                                                .groupby('day').sum()\\\n",
    "                                                .orderBy('day')\\\n",
    "                                                .toDF(*['day']+list(symptoms_dict_fr.keys())+['has_symptom'])\n",
    "\n",
    "tweets_covid_related = tweets_covid_related.select(['day'] +\n",
    "                                                  [F.col(c).cast('long') for c in cols_covid+['contains_symptom']]) \\\n",
    "                                                   .groupby('day').sum()\\\n",
    "                                                   .orderBy('day')\\\n",
    "                                                   .toDF(*['day']+cols_covid+['has_symptom'])\n",
    "\n",
    "list_covid_symptoms = tweets.filter(tweets['contains_symptom']==1)\\\n",
    "                            .select(['id_str','day','anonymized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to Pandas: number of tweets mentioning symptoms...\n",
      "Done in : 314sec\n",
      "Converting to Pandas: number of tweets mentioning symptoms+pronouns...\n",
      "Done in :284sec\n",
      "Converting covid related terms df to Pandas...\n",
      "Done in :199sec\n",
      "Converting tweets mentioning symptoms to Pandas...\n",
      "Done in: 237sec\n"
     ]
    }
   ],
   "source": [
    "# To Pandas\n",
    "start=time()\n",
    "print('Converting to Pandas: number of tweets mentioning symptoms...')\n",
    "tweets_mention_symptoms = tweets_mention_symptoms.withColumn('day', F.to_timestamp(tweets_mention_symptoms.day, 'yyy-MM-dd')).toPandas()\n",
    "print('Done in : ' + str(round(time()-start)) + 'sec')\n",
    "\n",
    "start=time()\n",
    "print('Converting to Pandas: number of tweets mentioning symptoms+pronouns...')\n",
    "tweets_has_symptoms = tweets_has_symptoms.withColumn('day', F.to_timestamp(tweets_has_symptoms.day, 'yyy-MM-dd')).toPandas()\n",
    "print('Done in :' + str(round(time()-start)) + 'sec')\n",
    "\n",
    "start=time()\n",
    "print('Converting covid related terms df to Pandas...')\n",
    "tweets_covid_related = tweets_covid_related.withColumn('day', F.to_timestamp(tweets_covid_related.day, 'yyy-MM-dd')).toPandas()\n",
    "print('Done in :' + str(round(time()-start)) + 'sec')\n",
    "\n",
    "start=time()\n",
    "print('Converting tweets mentioning symptoms to Pandas...')\n",
    "list_covid_symptoms = list_covid_symptoms.withColumn('day', F.to_timestamp(list_covid_symptoms.day, 'yyy-MM-dd')).toPandas()\n",
    "print('Done in: ' + str(round(time()-start)) + 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cleaning data for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading emergency file\n",
      "Downloading deaths file\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "emergencies_data_path = os.path.join(path_to_external_data, \"sante-publique-france\", 'emergencies_dataset.csv')\n",
    "source='https://www.data.gouv.fr/fr/datasets/r/eceb9fb4-3ebc-4da3-828d-f5939712600a'\n",
    "req = requests.get(source)\n",
    "url_content = req.content\n",
    "csv_file = open(emergencies_data_path, 'wb')\n",
    "csv_file.write(url_content)\n",
    "csv_file.close()\n",
    "print('Downloading emergency file')\n",
    "\n",
    "open_covid_path = os.path.join(path_to_external_data, \"sante-publique-france\", 'open_covid.csv')\n",
    "source = 'https://www.data.gouv.fr/en/datasets/r/0b66ca39-1623-4d9c-83ad-5434b7f9e2a4'\n",
    "req = requests.get(source)\n",
    "url_content = req.content\n",
    "csv_file = open(open_covid_path, 'wb')\n",
    "csv_file.write(url_content)\n",
    "csv_file.close()\n",
    "print('Downloading deaths file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading data from Santé Publique France\n",
    "emergencies = pd.read_csv(emergencies_data_path, sep=\";\")\n",
    "emergencies['date_de_passage'] = pd.to_datetime(emergencies['date_de_passage'])\n",
    "emergencies['dep']=emergencies['dep'].astype(str)\n",
    "\n",
    "# Keep only Île-De-France\n",
    "emergencies = emergencies.loc[emergencies['dep'].isin(['75','77','78','91','93','94','95'])].groupby(['date_de_passage']).agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deaths file\n",
    "open_covid = pd.read_csv(os.path.join(open_covid_path), sep=',')\n",
    "open_covid = open_covid[open_covid['maille_nom']=='Île-de-France']\n",
    "open_covid = open_covid.groupby('date').agg('mean').reset_index()\n",
    "\n",
    "# deaths in cumulative : change to frequency\n",
    "z = np.array(open_covid['deces'])\n",
    "z[1:] -= z[:-1].copy()\n",
    "open_covid['deaths_freq'] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling mean\n",
    "def rolling_mean(ts, window):\n",
    "    return ts.rolling(window=window).mean()\n",
    "\n",
    "def rolling_mean_df(df) :\n",
    "    df['has_symptom_mean_week'] = rolling_mean(df['has_symptom'], window=7)\n",
    "    df['has_symptom_mean_3'] = rolling_mean(df['has_symptom'], window=3)\n",
    "    return df\n",
    "\n",
    "for df in [tweets_mention_symptoms, tweets_has_symptoms]:\n",
    "    df=rolling_mean_df(df)\n",
    "    \n",
    "    \n",
    "emergencies_dict = {'nbre_pass_corona' : 'Nb passages emergencies',\n",
    "                   'nbre_hospit_corona' : 'Nb hospitalizations',\n",
    "                   'nbre_acte_corona' : 'Nb medical acts'}\n",
    "for type_urgence in list(emergencies_dict.keys()):\n",
    "    emergencies[type_urgence+'_mean_week'] = rolling_mean(emergencies[type_urgence], window=7)\n",
    "    emergencies[type_urgence+'_mean_3'] = rolling_mean(emergencies[type_urgence], window=3)\n",
    "    \n",
    "open_covid['deaths_3'] = rolling_mean(open_covid['deaths_freq'], window=3)\n",
    "open_covid['deaths_week'] = rolling_mean(open_covid['deaths_freq'], window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_mention_symptoms.to_csv(os.path.join(path_to_data,'visualisation_data','tweets_mention_symptoms.csv'), sep=';')\n",
    "tweets_has_symptoms.to_csv(os.path.join(path_to_data,'visualisation_data','tweets_has_symptoms.csv'), sep=';')\n",
    "emergencies.to_csv(os.path.join(path_to_data,'visualisation_data','emergencies.csv'), sep=';')\n",
    "open_covid.to_csv(os.path.join(path_to_data,'visualisation_data','open_covid.csv'), sep=';')\n",
    "list_covid_symptoms.to_csv(os.path.join(path_to_data,'visualisation_data','list_covid_symptoms.csv'),sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
