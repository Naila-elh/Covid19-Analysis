{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import socket\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import unidecode\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from py4j.java_gateway import java_import\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Local SparkSession\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print('Create Local SparkSession')\n",
    "    spark=SparkSession.builder.config(\"spark.driver.host\", \"localhost\").appName(\"extract-timelines\").getOrCreate()\n",
    "    \n",
    "# IgnoreCorruptFiles\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: FR\n",
      "Language: fr\n"
     ]
    }
   ],
   "source": [
    "country_code=\"FR\"\n",
    "language_code=\"fr\"\n",
    "print('Country:',country_code)\n",
    "print('Language:',language_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'fr' : 'French',\n",
    "                'en' : 'English',\n",
    "                'es' : 'Spanish',\n",
    "                'pt' : 'Portuguese'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "path_to_data = \"../data/\"\n",
    "path_to_parquets = os.path.join(path_to_data,'chunks','IDF-departments-to-analyze')\n",
    "parquet_files = sorted([x for x in Path(path_to_parquets).glob(\"**/*.parquet\")])\n",
    "path_to_external_data = os.path.join(path_to_data, \"external-data/\")\n",
    "path_to_output = os.path.join(path_to_data,'visualisation_data','tweets_symptoms_medra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List files to be processed...\n",
      "# Files: 16\n"
     ]
    }
   ],
   "source": [
    "print('List files to be processed...')\n",
    "\n",
    "fs=spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "list_status=fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(path_to_parquets))\n",
    "\n",
    "paths=[file.getPath().toString() for file in list_status]\n",
    "np.random.seed(0)\n",
    "paths=np.random.permutation(sorted(parquet_files))\n",
    "\n",
    "print('# Files:', len(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Read tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=spark.read.option(\"encoding\", \"UTF-8\").parquet(path_to_parquets)\n",
    "#tweets=spark.read.option(\"encoding\", \"UTF-8\").parquet(path_to_parquets+\"/0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets : 29647175\n",
      "Number of unique users : 30651\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets : %d\" % tweets.count())\n",
    "print(\"Number of unique users : %d\" % tweets.select('user_id').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets=tweets.filter(tweets.user_id=='1000008320369950720')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Clean tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only useful columns, and remove the accents and the punctuation. We stem all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIDECODE : functions to remove accents\n",
    "def make_trans():\n",
    "    matching_string = \"\"\n",
    "    replace_string = \"\"\n",
    "\n",
    "    for i in range(ord(\" \"), sys.maxunicode):\n",
    "        name = unicodedata.name(chr(i), \"\")\n",
    "        if \"WITH\" in name:\n",
    "            try:\n",
    "                base = unicodedata.lookup(name.split(\" WITH\")[0])\n",
    "                matching_string += chr(i)\n",
    "                replace_string += base\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    return matching_string, replace_string\n",
    "\n",
    "def clean_text(c):\n",
    "    matching_string, replace_string = make_trans()\n",
    "    return F.translate(\n",
    "        F.regexp_replace(c, \"\\p{M}\", \"\"), \n",
    "        matching_string, replace_string\n",
    "    ).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess tweets\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    #tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \n",
    "    df=df.select(\n",
    "            'user_id',\n",
    "            F.date_format(F.col('created_at'),\"yyyy-MM-dd\").alias('day').cast(\"date\"),\n",
    "            #'year','month','day',\n",
    "            F.lower(F.col('full_text')).alias('text'),\n",
    "            #F.lower(F.col('text')),\n",
    "            'lang'\n",
    "            )\n",
    "    \n",
    "#     df=df.withColumn(\"merge\", concat_ws(\"-\", $\"year\", $\"month\", $\"day\"))\\\n",
    "#         .withColumn(\"day\", to_date(unix_timestamp($\"merge\", \"yyyy-MM-dd\").cast(\"date\")))\\\n",
    "#         .drop(\"merge\")\n",
    "\n",
    "    df = df.repartition(160)\n",
    "\n",
    "    # remove retweets\n",
    "    df = df.filter(~ df.text.startswith('rt'))\n",
    "   \n",
    "    # identify userid, url, etc.\n",
    "    pre_process_udf = F.udf(lambda x:text_processor.pre_process_doc(x))\n",
    "    df=df.withColumn('text', pre_process_udf('text'))\n",
    "    \n",
    "    # language : french\n",
    "    df = df.filter(df.lang==language_code)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = clean_dataset(tweets)\n",
    "\n",
    "# Clean text: remove accents\n",
    "tweets = tweets.select('user_id', 'day', 'text', clean_text('text').alias('clean_text'))\n",
    "# Cleaning: remove and punctuation-smileys\n",
    "tweets = tweets.withColumn('clean_text', F.regexp_replace('clean_text', '[^\\sa-zA-Z0-9@#<>]', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214959\n"
     ]
    }
   ],
   "source": [
    "print(tweets.filter(tweets.clean_text.startswith('je')).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------+--------------------+\n",
      "|            user_id|       day|                text|          clean_text|\n",
      "+-------------------+----------+--------------------+--------------------+\n",
      "|1036925143036121092|2019-12-01|<user> comme qui ...|<user> comme qui ...|\n",
      "|1001101575643910144|2020-04-29|juste pour vous r...|juste pour vous r...|\n",
      "|1000040081527312386|2020-01-14|<user> au repos j...|<user> au repos j...|\n",
      "|1012688936081248257|2020-04-19|<user> <user> pas...|<user> <user> pas...|\n",
      "|         1042047907|2020-04-06|    réellement <url>|    reellement <url>|\n",
      "+-------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "for col in ['text','clean_text'] :\n",
    "\n",
    "    # Tokenize\n",
    "    tokenizer=TextPreProcessor(tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "                            dicts=[emoticons])\n",
    "    tokenize_udf = F.udf(lambda x:tokenizer.pre_process_doc(x), ArrayType(StringType()))\n",
    "    tweets=tweets.withColumn('tokens_'+col, tokenize_udf(col))\n",
    "    #tokenizer = Tokenizer(inputCol=col, outputCol='tokens_'+col)\n",
    "    #tweets = tokenizer.transform(tweets)\n",
    "\n",
    "    # Stem tokens\n",
    "    stemmer = SnowballStemmer(language=language_dict.get(language_code).lower())\n",
    "    stemmer_udf = F.udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "    tweets = tweets.withColumn(\"tokens_stem_\"+col, stemmer_udf('tokens_'+col))\n",
    "\n",
    "    # Join stemmed tokens\n",
    "    join_udf = F.udf(lambda x: \" \".join(x), StringType())\n",
    "    tweets = tweets.withColumn('stemmed_'+col, join_udf(F.col(\"tokens_stem_\"+col)))\n",
    "     #tweets = tweets.withColumn(\"stemmed_text\", F.concat_ws(\" \", \"tokens_stem\"))\n",
    "tweets = tweets.select('user_id','day','text','clean_text','stemmed_text','stemmed_clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Symptoms analysis based on MedRa dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "medra=spark.read.format('csv').option('header','true').load(os.path.join(path_to_external_data,'MedRa','medra_lang','medra_'+str(language_code)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_symptoms(df,version):\n",
    "    \n",
    "    print('Version : ' + str(version))\n",
    "    \n",
    "    symptoms = [row.pt_name for row in medra.select('pt_name').distinct().collect()]\n",
    "    all_llt_cols = [row.llt_name_as_col for row in medra.select('llt_name_as_col').distinct().collect()]\n",
    "    all_pt_codes = [row.pt_code for row in medra.select('pt_code').distinct().collect()]\n",
    "    v = versions.get(version)\n",
    "        \n",
    "    for symptom in symptoms :\n",
    "        targets = [row[v+'llt_name'] for row in medra.filter(medra['pt_name']==symptom).select(v+'llt_name').collect()]\n",
    "        targets_col = [row['llt_name_as_col'] for row in medra.filter(medra['pt_name']==symptom).select('llt_name_as_col').collect()]\n",
    "        symptom_code = medra.filter(medra['pt_name']==symptom).select('pt_code').collect()[0]['pt_code']\n",
    "        \n",
    "        for target in targets:\n",
    "            target_col_name=medra.filter(medra[v+'llt_name']==target).select('llt_name_as_col').collect()[0]['llt_name_as_col']\n",
    "            df=df.withColumn('llt_'+target_col_name, F.col(v+'text').contains(target).cast('int'))\n",
    "            \n",
    "        #df=df.withColumn(symptom, F.col(v+'text').rlike(\"|\".join(targets)).cast('int'))\n",
    "        df=df.withColumn('symptom_counts', sum(df['llt_'+c].cast('long') for c in targets_col))\n",
    "        df=df.withColumn('pt_'+symptom, F.col('symptom_counts')>=1)\n",
    "        df=df.withColumn('pt_'+symptom, F.col('pt_'+symptom).cast('int'))\n",
    "        df=df.withColumn('pt_code_'+symptom_code, F.col('pt_'+symptom))\n",
    "\n",
    "\n",
    "    df = df.withColumn('nb_symptoms', sum(df['pt_'+c].cast('long') for c in symptoms))\n",
    "    df = df.withColumn('contains_symptom', F.col('nb_symptoms')>=1)\n",
    "    df = df.withColumn('contains_symptom', F.col('contains_symptom').cast('int'))\n",
    "    \n",
    "    \n",
    "    # groupbyday\n",
    "    #df = df.filter(df.contains_symptom==1)\n",
    "    cols_symptoms_to_keep = ['contains_symptom']+['llt_' + x for x in all_llt_cols]+['pt_'+x for x in symptoms]+['pt_code_'+x for x in all_pt_codes]\n",
    "    df_symptoms = df.select(['day']+cols_symptoms_to_keep)\\\n",
    "            .groupby('day')\\\n",
    "            .agg(*[F.sum(x).alias(x) for x in cols_symptoms_to_keep], F.count('*').alias('nb_tweets'))\\\n",
    "            .orderBy('day')\n",
    "    \n",
    "    return df_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version : V0\n"
     ]
    }
   ],
   "source": [
    "versions = {'V0' : '',\n",
    "           'V1' : 'clean_',\n",
    "           'V2' : 'stemmed_',\n",
    "           'V3' : 'stemmed_clean_'}\n",
    "\n",
    "for version in versions.keys():\n",
    "    tweets_symptoms = count_number_symptoms(tweets, version)\n",
    "    tweets_symptoms = tweets_symptoms.withColumn('day', F.to_timestamp(tweets_symptoms.day, 'yyyy-MM-dd')).toPandas()\n",
    "    tweets_symptoms.to_csv(os.path.join(path_to_output,'tweets_symptoms_'+str(country_code)+'_'+str(version)+'.csv'), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_number_symptoms(df,version):\n",
    "    \n",
    "#     print('Version : ' + str(version))\n",
    "    \n",
    "#     symptoms = [row.pt_name for row in medra.select('pt_name').distinct().collect()]\n",
    "#     v = versions.get(version)\n",
    "        \n",
    "#     for symptom in symptoms :\n",
    "#         targets = [row[v+'llt_name'] for row in medra.filter(medra['pt_name']==symptom).select(v+'llt_name').collect()]\n",
    "#         df=df.withColumn(symptom, F.col(v+'text').rlike(\"|\".join(targets)).cast('int'))\n",
    "\n",
    "#     df = df.withColumn('nb_symptoms', sum(df[c].cast('long') for c in symptoms))\n",
    "#     df = df.withColumn('contains_symptom', F.col('nb_symptoms')>=1)\n",
    "#     df = df.withColumn('contains_symptom', F.col('contains_symptom').cast('int'))\n",
    "    \n",
    "    \n",
    "#     # groupbyday\n",
    "#     df = df.filter(df.contains_symptom==1)\n",
    "#     df_symptoms = df.select(['day','contains_symptom'])\\\n",
    "#             .groupby('day').sum()\\\n",
    "#             .orderBy('day')\\\n",
    "#             .toDF(*['day', 'contains_symptom_'+str(version)])\n",
    "    \n",
    "#     return df_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versions = {'V0' : '',\n",
    "#            'V1' : 'clean_',\n",
    "#            'V2' : 'stemmed_',\n",
    "#            'V3' : 'stemmed_clean_'}\n",
    "\n",
    "# for version in versions.keys():\n",
    "#     if version=='V0':\n",
    "#         tweets_symptoms = count_number_symptoms(tweets, version)\n",
    "#     else:\n",
    "#         df = count_number_symptoms(tweets, version)\n",
    "#         tweets_symptoms = tweets_symptoms.join(df, on=['day'], how='full_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export table to csv\n",
    "# tweets_symptoms = tweets_symptoms.withColumn('day', F.to_timestamp(tweets_symptoms.day, 'yyyy-MM-dd')).toPandas()\n",
    "# tweets_symptoms.to_csv(os.path.join(path_to_output,'tweets_symptoms_'+str(country_code)+'.csv'), header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
