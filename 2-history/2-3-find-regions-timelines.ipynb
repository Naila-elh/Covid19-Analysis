{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from time import time\n",
    "import socket\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,when,count,col,count,lit,sum\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType\n",
    "from py4j.java_gateway import java_import\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Local SparkSession\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print('Create Local SparkSession')\n",
    "    spark=SparkSession.builder.config(\"spark.driver.host\", \"localhost\").appName(\"extract-timelines\").getOrCreate()\n",
    "    \n",
    "# IgnoreCorruptFiles\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "path_to_data = \"../data/\"\n",
    "path_to_timeline=os.path.join(path_to_data,'timelines/API/IDF_departments/')\n",
    "#path_to_timeline=os.path.join(path_to_data,'timelines/API/IDF/')\n",
    "path_to_external_data = os.path.join(path_to_data, \"external-data/\")\n",
    "path_to_parquets = os.path.join(path_to_data,'chunks','IDF_departments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "parquet_files = sorted([x for x in Path(path_to_parquets).glob(\"**/*.parquet\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(index_block,n_blocks):\n",
    "    df=pd.DataFrame()\n",
    "    for parquet_file in parquet_files[n_blocks*index_block:n_blocks*(index_block+1)]:\n",
    "        #print(str(json_file))\n",
    "        timeline=pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "        df = pd.concat([df,timeline])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE DEFAULT # CORES\n",
      "# PROCESSORS: 16 \n",
      "\n",
      "Read Tweets ...\n",
      "125\n",
      "[<multiprocessing.pool.ApplyResult object at 0x7fe86a149a10>, <multiprocessing.pool.ApplyResult object at 0x7fe86a149cd0>, <multiprocessing.pool.ApplyResult object at 0x7fe86a149d90>, <multiprocessing.pool.ApplyResult object at 0x7fe86a149e50>, <multiprocessing.pool.ApplyResult object at 0x7fe86a149f10>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f090>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f150>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f210>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f2d0>, <multiprocessing.pool.ApplyResult object at 0x7fe86a149fd0>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f3d0>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f490>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f550>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f610>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f6d0>, <multiprocessing.pool.ApplyResult object at 0x7fe86a14f790>]\n",
      "done\n",
      "DONE IN 443 SEC\n"
     ]
    }
   ],
   "source": [
    "# Parallelization\n",
    "n_cpu = mp.cpu_count()\n",
    "print('USE DEFAULT # CORES')\n",
    "pool  = mp.Pool(processes=n_cpu)\n",
    "print(\"# PROCESSORS:\", n_cpu, \"\\n\")\n",
    "\n",
    "print(\"Read Tweets ...\")\n",
    "start = time()\n",
    "\n",
    "# COMPUTE LIST OF RESULTS\n",
    "n_blocks= len(parquet_files)//n_cpu + len(parquet_files)%n_cpu\n",
    "print(n_blocks)\n",
    "results = [pool.apply_async(get_tweets, args=(index_block,n_blocks)) for index_block in range(n_cpu)]\n",
    "print(results)\n",
    "tweets  = pd.concat([results[index_block].get() for index_block in range(n_cpu)]).reset_index(drop=True)\n",
    "print('done')\n",
    "\n",
    "pool.close()    \n",
    "print(\"DONE IN\", round(time() - start), \"SEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop_duplicates(subset='id_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10055191"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets['user_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning localisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. To geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://public.opendatasoft.com/explore/dataset/contours-geographiques-des-departements-2019/export/\n",
    "departments = pd.read_csv('https://public.opendatasoft.com/explore/dataset/contours-geographiques-des-departements-2019/download/?format=csv&timezone=Europe/Berlin&lang=fr&use_labels_for_header=true&csv_separator=%3B', sep=';')\n",
    "departments = departments.rename(columns={'Nom du département (MAJUSCULE)' : 'Department name',\n",
    "                               'Nom de la région (MAJUSCULE)' : 'Region name'})\n",
    "departments = departments[['Geo Shape','Department name','Region name','Code INSEE Département','Code INSEE Région']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departments - to geopandas\n",
    "departments['Geo Shape'] = departments['Geo Shape'].apply(lambda x: json.loads(x))\n",
    "departments['Geo Shape'] = departments['Geo Shape'].apply(lambda x: shape(x))\n",
    "departments_geoshape = gpd.GeoDataFrame(departments).set_geometry('Geo Shape')\n",
    "\n",
    "# Departments : compute area\n",
    "departments_geoshape['area'] = departments_geoshape.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regions\n",
    "regions = departments_geoshape.dissolve(by='Region name').reset_index()\n",
    "regions = regions[['Region name','Geo Shape','Code INSEE Région']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Get location of each tweet (when possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the location of a tweet, we compute the area and the centroid of the bounding box related to the location. We merge the tweets df with the geopandas df related to the department data -- only for the tweets where the location is small enough (so that we know it is capturing cities and not regions or countries. And we look if the centroid is in the department / region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tweets to geopandas\n",
    "tweets['coordinates'] = tweets.loc[tweets['coordinates'].notna(),'coordinates'].apply(lambda x : [item for sublist in x for item in sublist])\n",
    "tweets['coordinates'] = tweets.loc[tweets['coordinates'].notna(), 'coordinates'].apply(lambda x: Polygon(x))\n",
    "\n",
    "# tweets dep\n",
    "tweets_geo = tweets.loc[~tweets['coordinates'].isna()]\n",
    "tweets_geo = gpd.GeoDataFrame(tweets_geo).set_geometry('coordinates')\n",
    "tweets_geo['area'] = tweets_geo.area\n",
    "tweets_geo['centroid'] = tweets_geo.centroid\n",
    "tweets_geo = gpd.GeoDataFrame(pd.DataFrame(tweets_geo)).set_geometry('centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining with departments - centroid...\n",
      "Joining with regions...\n"
     ]
    }
   ],
   "source": [
    "print('Joining with departments - centroid...')\n",
    "tweets_dep = tweets_geo[tweets_geo['area'] < departments_geoshape['area'].mean()]\n",
    "tweets_dep = sjoin(tweets_dep, departments_geoshape, how='inner', op='within')\n",
    "\n",
    "print('Joining with regions...')\n",
    "tweets_reg = tweets_geo.loc[(~tweets_geo['id_str'].isin(tweets_dep['id_str'].tolist())) & \n",
    "                            (tweets_geo['city'] == 'France')]\n",
    "tweets_reg = sjoin(tweets_reg, regions, how='left', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final = pd.concat([tweets_dep, tweets_reg])\n",
    "tweets_final = pd.concat([tweets_final, tweets[~tweets['id_str'].isin(tweets_final['id_str'].tolist())]]) \\\n",
    "                        .sort_values(by=['user_id','created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extrapolate user's location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lockdown = tweets_final[tweets['created_at'] > '02-01-2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tweets_lockdown.groupby('user_id')['Department name'].agg({'value_counts'}).reset_index()\n",
    "users = users.sort_values(by=['user_id','value_counts'], ascending=True) \\\n",
    "            .drop_duplicates(subset='user_id', keep='last') \\\n",
    "            .rename(columns = {'Department name' : 'Department'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_final = pd.merge(tweets_final, users, on='user_id', how='left')\n",
    "tweets_final = pd.DataFrame(tweets_final, columns=['id_str','created_at','full_text','lang','user_id','user_name','city',\n",
    "                                                   'Department','Code INSEE Département'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_parquets = os.path.join(path_to_data,'chunks','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write chunk 0\n",
      "Write chunk 1\n",
      "Write chunk 2\n",
      "Write chunk 3\n",
      "Write chunk 4\n",
      "Write chunk 5\n",
      "Write chunk 6\n",
      "Write chunk 7\n",
      "Write chunk 8\n",
      "Write chunk 9\n"
     ]
    }
   ],
   "source": [
    "n_chunks = mp.cpu_count()\n",
    "n_blocks = len(tweets_final)//n_chunks + len(tweets_final)%n_chunks\n",
    "\n",
    "for i_chunk in range (n_chunks):\n",
    "    print('Write chunk ' + str(i_chunk))\n",
    "    df = tweets_final[n_blocks*i_chunk:n_blocks*(i_chunk+1)]\n",
    "    df.to_parquet(os.path.join(path_to_parquets, str(i_chunk)+ '.parquet'), engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
